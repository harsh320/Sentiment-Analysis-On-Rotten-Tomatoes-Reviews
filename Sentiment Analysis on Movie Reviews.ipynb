{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip',delimiter='\\t')\ndf_test = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/test.tsv.zip',delimiter='\\t')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.dropna()\ndf_test = df_test.dropna()\nX_train = df_train['Phrase'].values\nY_train = df_train['Sentiment'].values\nX_test = df_test['Phrase'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train['Sentiment'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport nltk\ndef clean_text(line):\n    line = line.lower()\n    no_punct = [words for words in line if words not in string.punctuation]\n    line = ''.join(no_punct)\n    line = line.split()\n    line = [words for words in line if words not in nltk.corpus.stopwords.words('english')]\n    line = ''.join(line)\n    return line","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()\n\ndef clean_text_stem(line):\n    line = line.lower()\n    no_punct = [words for words in line if words not in string.punctuation]\n    line = ''.join(no_punct)\n    line = line.split()\n    line = [lemmatizer.lemmatize(words) for words in line if words not in nltk.corpus.stopwords.words('english')]\n    line = ''.join(line)\n    return line","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\nX_train_clean = [clean_text_stem(line) for line in tqdm.tqdm(X_train)]\nX_test_clean = [clean_text_stem(line) for line in tqdm.tqdm(X_test)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **BAG OF WORDS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nvectorizer.fit(X_train_clean)\nX_train_cv = vectorizer.transform(X_train_clean)\nX_test_cv = vectorizer.transform(X_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_cv,X_val_cv,Y_train_cv,Y_val_cv = train_test_split(X_train_cv,Y_train,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nnb_cv =  MultinomialNB().fit(X_train_cv, Y_train_cv)\nclf = LogisticRegression(random_state=0,solver='lbfgs',class_weight='balanced', max_iter=10000).fit(X_train_cv, Y_train_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nY_pred_cv = clf.predict(X_val_cv)\nprint(accuracy_score(Y_val_cv, Y_pred_cv))\nY_pred_cv = nb_cv.predict(X_val_cv)\nprint(accuracy_score(Y_val_cv, Y_pred_cv))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **TF-IDF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntfv.fit(X_train_clean)\nX_train_clean_tf = tfv.transform(X_train_clean)\nX_test_clean_tf = tfv.transform(X_test_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_tf,X_val_tf,Y_train_tf,Y_val_tf = train_test_split(X_train_clean_tf,Y_train,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_tf = LogisticRegression(random_state=0,solver='lbfgs',class_weight='balanced', max_iter=10000).fit(X_train_tf, Y_train_tf)\nnb_tf =  MultinomialNB().fit(X_train_tf, Y_train_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_tf = clf_tf.predict(X_val_tf)\nprint(accuracy_score(Y_val_tf, Y_pred_tf))\nY_pred_tf = nb_tf.predict(X_val_tf)\nprint(accuracy_score(Y_val_tf, Y_pred_tf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LSTM WORD2VEC**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nt = Tokenizer()\nt.fit_on_texts(X_train_clean)\nX_train_token = t.texts_to_sequences(X_train_clean)\nX_test_token = t.texts_to_sequences(X_test_clean)\nsent_length = 50\nX_train_token_pad = pad_sequences(X_train_token,padding='pre',maxlen=sent_length)\nX_test_token_pad = pad_sequences(X_test_token,padding='pre',maxlen=sent_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Embedding\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense,SpatialDropout1D,Dropout\nembedding_vector_features=100\nvoc_size = len(t.word_index)+1\nmodel=Sequential()\n\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nX_train_token_pad, X_val_token_pad, Y_train_token_pad, Y_val_token_pad = train_test_split(X_train_token_pad, Y_train, test_size=0.33, random_state=42)\nY_train_token_pad = np_utils.to_categorical(Y_train_token_pad)\nY_val_token_pad = np_utils.to_categorical(Y_val_token_pad)\nmodel.fit(X_train_token_pad,Y_train_token_pad,validation_data=(X_val_token_pad,Y_val_token_pad),epochs=8,batch_size=64)\nY_pred_token_pre = model.predict_classes(X_test_token_pad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LSTM pre-trained WORD2VEC**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\ndef get_vocab(data):\n    vocab = {}\n    for line in tqdm.tqdm(data):\n        line = line.split()\n        for word in line:\n            try:\n                vocab[word] += 1\n            except:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = get_vocab(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim.downloader as api\npath = api.load(\"word2vec-google-news-300\", return_path=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\nmodel_google = KeyedVectors.load_word2vec_format(path, binary = True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator \n\ndef check_coverage(voc,model):\n    present = {}\n    not_present = {}\n    k = 0\n    i = 0\n    for word in (voc):\n        try:\n            present[word] = model[word]\n            k += voc[word]\n        except:\n            not_present[word] = voc[word]\n            i += voc[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(present) / len(voc)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(not_present.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_present = check_coverage(vocab,model_google)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_present","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text_w2v(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_clean_w2v = [clean_text_w2v(line) for line in tqdm.tqdm(X_train)]\nvocab = get_vocab(X_train_clean_w2v)\nnot_present = check_coverage(vocab,model_google)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_clean_w2v = [clean_text_w2v(line) for line in tqdm.tqdm(X_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM =300\nword_index = t.word_index\nnum_words = len(t.word_index) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    if i > num_words:\n        continue\n    try:\n        embedding_vector = model_google[word]\n        embedding_matrix[i] = embedding_vector\n    except:\n        None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nt = Tokenizer()\nt.fit_on_texts(X_train_clean_w2v)\nX_train_token = t.texts_to_sequences(X_train_clean_w2v)\nX_test_token = t.texts_to_sequences(X_test_clean_w2v)\nsent_length = 50\nX_train_token_pad = pad_sequences(X_train_token,padding='pre',maxlen=sent_length)\nX_test_token_pad = pad_sequences(X_test_token,padding='pre',maxlen=sent_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_token_pad, X_val_token_pad, Y_train_token_pad, Y_val_token_pad = train_test_split(X_train_token_pad, Y_train, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train_token_pad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\nY_train_token_pad = np_utils.to_categorical(Y_train_token_pad)\nY_val_token_pad = np_utils.to_categorical(Y_val_token_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten,Dropout,LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.initializers import Constant\nmax_length = 50\nmodel = Sequential()\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=max_length,\n                            trainable=False)\n\n\nmodel.add(embedding_layer)\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())\n\n\n# fit the model\nmodel.fit(X_train_token_pad, Y_train_token_pad, batch_size=128, epochs=32, validation_data=(X_val_token_pad,Y_val_token_pad), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_token_pre = model.predict_classes(X_test_token_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['PhraseId'] = df_test.PhraseId\nsubmission['Sentiment'] = Y_pred_token_pre\nsubmission.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}